# Data
data_depth: 6
data_repeat: 2
data_sample_ancestors: True
data_min_test_depth: 3
data_batch_size: 256

# Shared
dim_z: 8
vocab_size: 12
max_w_length: 6
vocab_group_size: 2

save_prefix: 'discretizer' # Prefix for the saved model files.
save_checkpoint_every: 10000 # How often to save checkpoints (in number of updates)

# Reward
score_weight: 1
decodability_weight: 1
length_penalty: 0
min_log_reward: -40

# Discretizer
discretizer_objective: 'subtb' # 'tb', 'subtb', 'db'
discretizer_dim_h: 1024
discretizer_num_layers: 3

# Flow models
flow_dim_h: 1024
flow_num_layers: 3

# M-Model
m_model_dim_h: 256
m_model_num_layers: 3
m_model_num_w_embedding_layers: 1
m_model_vae_beta: 1
m_model_cvae_beta: 1

# Training
lr_discretizer: 0.0001
lr_flows: 0.0001
lr_m_step: 0.0001
p_sleep_phase: 0 # probability of sleeping during training
p_explore_discretizer: .05

# EM
start_e_steps: 0 # number of e-steps to start with, ignoring all other conditions
min_e_steps: 100 # number of e-steps to perform before checking loss-based conditions
max_e_steps: 100 # If number of e-steps exceeds this, switch to M-step regardless of loss-based conditions.
num_m_steps: 50
m_step_temperature: 1 # temperature for sampling P(w|z) during M-step
m_step_argmax: true # if True, uses argmax instead of sampling P(w|z) during M-step
m_step_p_explore: 0 # probability of exploring during M-step
m_step_substrings: true
m_step_max_w_length: true
