# Data
data_batch_size: 128
data_size: 32
data_min_scale: 0
data_holdout_xy_mode: True
data_holdout_xy_nonmode: False
data_holdout_xy_shape: False
data_holdout_xy_mode_color: True
data_holdout_shape_color: True

# Shared
dim_z: 8
vocab_size: 32
min_w_length: 1
max_w_length: 7

save_prefix: 'discretizer' # Prefix for the saved model files.
save_checkpoint_every: 10000 # How often to save checkpoints (in number of updates)
create_plots_every: 2500 # How often to create plots (in number of updates)

# Reward
score_weight: 1
decodability_weight: 0
length_penalty: 0
min_log_reward: -40

# Discretizer
discretizer_objective: 'subtb' # 'tb', 'subtb', 'db'
discretizer_dim_h: 1024
discretizer_num_layers: 3

# Flow models
flow_dim_h: 1024
flow_num_layers: 3

# M-Model
m_model_dim_h: 256
m_model_num_layers: 3
m_model_num_w_embedding_layers: 1
m_model_vae_beta: 1
m_model_cvae_beta: 1
m_model_vq_loss: true

# Training
lr_discretizer: 0.001
lr_flows: 0.001
lr_m_step: 0.0001
p_sleep_phase: 0 # probability of sleeping during training
p_explore_discretizer: .05

# EM
start_e_steps: 0 # number of e-steps to start with, ignoring all other conditions
min_e_steps: 100 # number of e-steps to perform before checking loss-based conditions
max_e_steps: 500000 # If number of e-steps exceeds this, switch to M-step regardless of loss-based conditions.
num_m_steps: 50
m_step_temperature: 1 # temperature for sampling P(w|z) during M-step
m_step_argmax: true # if True, uses argmax instead of sampling P(w|z) during M-step
m_step_p_explore: 0 # probability of exploring during M-step
m_step_substrings: true
m_step_max_w_length: true
e_step_unique_w_start: 0
e_step_unique_w_margin: 20

# Images
discretizer_encoder_dim_h: 64
m_model_patch_size: 8
m_model_use_mu_as_z0: true
m_model_sd_model_num_layers: 3
m_model_nhead: 8
m_model_dim_feedforward: 256
m_model_num_encoder_layers: 2
m_model_num_decoder_layers: 2